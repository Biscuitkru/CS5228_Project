{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# CS5228 Project: HDB Resale Price Prediction\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Biscuitkru/CS5228_Project/blob/main/CS5228_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Project Overview\n",
    "This notebook contains the complete analysis and modeling pipeline for predicting HDB (Housing & Development Board) resale prices in Singapore. The project implements various machine learning techniques including:\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Feature Engineering with proximity-based features\n",
    "- Multiple regression models (Linear, Polynomial, Tree-based)\n",
    "- Advanced gradient boosting methods (XGBoost)\n",
    "\n",
    "**Authors:** CS5228 Group 34  \n",
    "**Course:** Knowledge Discovery and Data Mining  \n",
    "**Semester:** AY2025/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FuepXAQuI_Z"
   },
   "outputs": [],
   "source": [
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q xgboost\n",
    "!pip install -q tqdm\n",
    "!pip install -q joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5opL5RffEW5"
   },
   "outputs": [],
   "source": [
    "import os, re, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple, Dict, List, Any\n",
    "from collections import Counter\n",
    "\n",
    "# for modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuQJdXpBq13E",
    "outputId": "13d4cb20-e6c1-4683-e240-7cbff8e8d058"
   },
   "outputs": [],
   "source": [
    "# # ########################################\n",
    "# # # Configs for kaggle run (upload the data folder as a zip)\n",
    "# # ########################################\n",
    "\n",
    "# # Main\n",
    "# train_path = \"/kaggle/input/cs5228-dataset-g34/data/train.csv\"\n",
    "# test_path = \"/kaggle/input/cs5228-dataset-g34/data/test.csv\"\n",
    "\n",
    "# # Auxiliary data\n",
    "# hdb_data_path = \"/kaggle/input/cs5228-dataset-g34/data/auxiliary-data/sg-hdb-block-details.csv\"\n",
    "# gov_hawkers_path = \"/kaggle/input/cs5228-dataset-g34/data/auxiliary-data/sg-gov-hawkers.csv\"\n",
    "# mrt_stations_path = \"/kaggle/input/cs5228-dataset-g34/data/auxiliary-data/sg-mrt-stations.csv\"\n",
    "# pri_schools_path = \"/kaggle/input/cs5228-dataset-g34/data/auxiliary-data/sg-primary-schools.csv\"\n",
    "# sec_schools_path = \"/kaggle/input/cs5228-dataset-g34/data/auxiliary-data/sg-secondary-schools.csv\"\n",
    "# shopping_malls_path = \"/kaggle/input/cs5228-dataset-g34/data/auxiliary-data/sg-shopping-malls.csv\"\n",
    "\n",
    "# # Modeling controls\n",
    "# seed = 777\n",
    "# rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kscCewndpa4x"
   },
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# # Configs for colab\n",
    "# ########################################\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Main\n",
    "# train_path = \"/content/drive/MyDrive/CS Masters/CS5228/train.csv\"\n",
    "# test_path = \"/content/drive/MyDrive/CS Masters/CS5228/test.csv\"\n",
    "\n",
    "# # Auxiliary data\n",
    "# hdb_data_path = \"/content/drive/MyDrive/CS Masters/CS5228/auxiliary-data/sg-hdb-block-details.csv\"\n",
    "# gov_hawkers_path = \"/content/drive/MyDrive/CS Masters/CS5228/auxiliary-data/sg-gov-hawkers.csv\"\n",
    "# mrt_stations_path = \"/content/drive/MyDrive/CS Masters/CS5228/auxiliary-data/sg-mrt-stations.csv\"\n",
    "# pri_schools_path = \"/content/drive/MyDrive/CS Masters/CS5228/auxiliary-data/sg-primary-schools.csv\"\n",
    "# sec_schools_path = \"/content/drive/MyDrive/CS Masters/CS5228/auxiliary-data/sg-secondary-schools.csv\"\n",
    "# shopping_malls_path = \"/content/drive/MyDrive/CS Masters/CS5228/auxiliary-data/sg-shopping-malls.csv\"\n",
    "\n",
    "# # Modeling controls\n",
    "# seed = 777\n",
    "# rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cyg9CKvU8Pz8"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Configs for local run\n",
    "###############################\n",
    "\n",
    "local_path = r\"C:/Users/GoCh661/Downloads/cs5228/data\"\n",
    "\n",
    "\n",
    "# Main\n",
    "train_path = local_path + \"/train.csv\"\n",
    "test_path = local_path + \"/test.csv\"\n",
    "\n",
    "# Auxiliary data\n",
    "hdb_data_path = local_path + \"/auxiliary-data/sg-hdb-block-details.csv\"\n",
    "gov_hawkers_path = local_path + \"/auxiliary-data/sg-gov-hawkers.csv\"\n",
    "mrt_stations_path = local_path + \"/auxiliary-data/sg-mrt-stations.csv\"\n",
    "pri_schools_path = local_path + \"/auxiliary-data/sg-primary-schools.csv\"\n",
    "sec_schools_path = local_path + \"/auxiliary-data/sg-secondary-schools.csv\"\n",
    "shopping_malls_path = local_path + \"/auxiliary-data/sg-shopping-malls.csv\"\n",
    "\n",
    "# Modeling controls\n",
    "seed = 777\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Standardize column names to lowercase\n",
    "train_df.columns = train_df.columns.str.lower()\n",
    "test_df.columns = test_df.columns.str.lower()\n",
    "\n",
    "print(\"Column names standardized to lowercase\")\n",
    "print(f\"Train columns: {list(train_df.columns)[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data loaded correctly\n",
    "print(\"=\"*70)\n",
    "print(\"DATA LOADING VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training data loaded from: {train_path}\")\n",
    "print(f\"Test data loaded from: {test_path}\")\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"\\nSample columns: {list(train_df.columns)[:5]}\")\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis (EDA) and Data Preprocessing\n",
    "\n",
    "This section provides comprehensive analysis of the HDB resale dataset, including:\n",
    "- Dataset overview and statistical summaries\n",
    "- Target variable distribution analysis\n",
    "- Feature correlation studies\n",
    "- Categorical and temporal pattern exploration\n",
    "- Data quality assessment\n",
    "\n",
    "The findings from this EDA guide our feature engineering and model selection strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Overview and Basic Statistics\n",
    "\n",
    "**Purpose:** Examine the basic structure, dimensions, and statistical properties of the training dataset.\n",
    "\n",
    "The following analysis provides:\n",
    "- Descriptive statistics for all numerical features\n",
    "- Data type distribution\n",
    "- Missing value assessment\n",
    "- Initial data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "p4gO4ExbFyLa",
    "outputId": "8f45c4ff-e627-4dcc-fb2a-c0e147296ab4"
   },
   "outputs": [],
   "source": [
    "train_df.describe() # No NA values at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ReYfAKVI8A6"
   },
   "outputs": [],
   "source": [
    "# Date Manipulation\n",
    "def process_month_column(df):\n",
    "    df['month'] = pd.to_datetime(df['month'], format=\"%Y-%m\")\n",
    "    df['sale_year'] = df['month'].dt.year\n",
    "    df['sale_month'] = df['month'].dt.month\n",
    "    # sale_quarter as a feature to capture potential seasonality in resale prices.\n",
    "    df['sale_quarter'] = df['month'].dt.quarter\n",
    "    return df\n",
    "\n",
    "train_df = process_month_column(train_df)\n",
    "test_df = process_month_column(test_df)\n",
    "\n",
    "# additional month_index to capture trend\n",
    "abs_month_train = train_df[\"sale_year\"]*12 + train_df[\"sale_month\"]\n",
    "abs_month_test  = test_df[\"sale_year\"]*12 + test_df[\"sale_month\"]\n",
    "train_df[\"month_index\"] = (abs_month_train - abs_month_train.min()).astype(float)\n",
    "test_df[\"month_index\"]  = (abs_month_test  - abs_month_train.min()).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-uuVSrbT4bT"
   },
   "outputs": [],
   "source": [
    "# To remove trailing whitespace and reduce multiple spaces to a single space\n",
    "def norm_text_basic(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# Normalise + uppercase, mainly for the str values\n",
    "def norm_key(s, case=\"upper\"):\n",
    "    s = norm_text_basic(s)\n",
    "    if isinstance(s, str):\n",
    "        return s.upper() if case == \"upper\" else s.casefold()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBtiHIZET97m"
   },
   "outputs": [],
   "source": [
    "for col in [\"town\",\"flat_type\", \"block\", \"street\", \"flat_model\"]:\n",
    "    if col in train_df.columns: train_df[col] = train_df[col].map(norm_key)\n",
    "    if col in test_df.columns: test_df[col]  = test_df[col].map(norm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWiapvSV-ES9"
   },
   "outputs": [],
   "source": [
    "# Standardize flat type\n",
    "def standardise_flat_type(val: str) -> str:\n",
    "    v = val.strip().replace(\"_\", \" \").replace(\"-\", \" \").replace(\"  \", \" \")\n",
    "    v = v.replace(\"ROOM\", \" ROOM\")\n",
    "    v = re.sub(r\"\\s+\", \" \", v).strip()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90f96cca",
    "outputId": "7fc43134-c7e8-4746-8e86-dba1e5f1018c"
   },
   "outputs": [],
   "source": [
    "train_df['flat_type'] = train_df['flat_type'].apply(standardise_flat_type)\n",
    "test_df['flat_type'] = test_df['flat_type'].apply(standardise_flat_type)\n",
    "print(train_df['flat_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMr_mhjcQTP5"
   },
   "outputs": [],
   "source": [
    "# General Price hierarchy for HDB flats\n",
    "flat_type_order = [\"1 ROOM\",\"2 ROOM\",\"3 ROOM\",\"4 ROOM\",\"5 ROOM\",\"MULTI GENERATION\",\"EXECUTIVE\"]\n",
    "order_map = {ft:i for i, ft in enumerate(flat_type_order)}\n",
    "\n",
    "train_df[\"flat_type_price_hier\"] = train_df[\"flat_type\"].map(lambda x: order_map.get(str(x).upper(), np.nan))\n",
    "test_df[\"flat_type_price_hier\"]  = test_df[\"flat_type\"].map(lambda x: order_map.get(str(x).upper(), np.nan))\n",
    "\n",
    "# For Decision trees\n",
    "for col, val in [(\"is_exec\",\"EXECUTIVE\"), (\"is_multigen\",\"MULTI GENERATION\")]:\n",
    "    train_df[col] = (train_df[\"flat_type\"].str.upper() == val).astype(int)\n",
    "    test_df[col]  = (test_df[\"flat_type\"].str.upper() == val).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9KRd2WDVfDs"
   },
   "outputs": [],
   "source": [
    "# No. of Bedrooms\n",
    "BEDROOMS_MAP = {\"1 ROOM\":0,\"2 ROOM\":1,\"3 ROOM\":2,\"4 ROOM\":3,\"5 ROOM\":4,\"EXECUTIVE\":3,\"MULTI GENERATION\":4}\n",
    "train_df[\"bedrooms_est\"] = train_df[\"flat_type\"].str.upper().map(BEDROOMS_MAP)\n",
    "test_df[\"bedrooms_est\"]  = test_df[\"flat_type\"].str.upper().map(BEDROOMS_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdS3s_G6DB-F"
   },
   "outputs": [],
   "source": [
    "# Floor Range, just going to set as the middle numnber\n",
    "def parse_floor_mid(floor_range) -> float:\n",
    "    if isinstance(floor_range, (float)):\n",
    "        return float(floor_range)\n",
    "    m = re.findall(r\"\\d{1,2}\", floor_range)\n",
    "    if len(m) >= 2:\n",
    "        lo, hi = int(m[0]), int(m[1])\n",
    "        if lo <= hi:\n",
    "            return (lo + hi) / 2.0\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcfpMdUSD8px",
    "outputId": "daaa91c5-f028-4b11-b54a-e3f7ea3ac618"
   },
   "outputs": [],
   "source": [
    "train_df['floor_range'] = train_df['floor_range'].apply(parse_floor_mid).astype(float)\n",
    "train_df = train_df.rename(columns={\"floor_range\": \"floor_mid\"})\n",
    "\n",
    "test_df['floor_range'] = test_df['floor_range'].apply(parse_floor_mid).astype(float)\n",
    "test_df = test_df.rename(columns={\"floor_range\": \"floor_mid\"})\n",
    "\n",
    "print(np.sort(train_df['floor_mid'].unique()))\n",
    "print(np.sort(test_df['floor_mid'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnsvG2lTEUyN",
    "outputId": "3f6659a1-384d-47ef-9229-5fbc45329ebb"
   },
   "outputs": [],
   "source": [
    "print(train_df['eco_category'].unique())\n",
    "print(test_df['eco_category'].unique())\n",
    "\n",
    "# Current dataset is taken directly from the Kaggle scoreboard which is missing eco_category (Might need to double check whether this is true)\n",
    "train_df = train_df.drop('eco_category', axis=1)\n",
    "test_df = test_df.drop('eco_category', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksQv094-QRMA"
   },
   "outputs": [],
   "source": [
    "# Flat age, years left from 99\n",
    "train_df = train_df.rename(columns={\"lease_commence_data\": \"lease_commence_date\"})\n",
    "test_df = test_df.rename(columns={\"lease_commence_data\": \"lease_commence_date\"})\n",
    "\n",
    "train_df[\"lease_left\"] = 99 - (train_df[\"sale_year\"] - train_df[\"lease_commence_date\"])\n",
    "test_df[\"lease_left\"] = 99 - (test_df[\"sale_year\"] - test_df[\"lease_commence_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3KZI6v9EgqU"
   },
   "outputs": [],
   "source": [
    "# HDB price drops faster as the remaining lease falls—use non-linear transforms and an approximate leasehold-relativity curve\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df[\"lease_left_sq\"]  = df[\"lease_left\"]**2\n",
    "    df[\"lease_left_log\"] = np.log1p(df[\"lease_left\"].clip(lower=0))\n",
    "\n",
    "    # Bala-style approximation using anchor points (convex decay)\n",
    "    anchors_years = np.array([30, 60, 99], dtype=float)\n",
    "    anchors_rel   = np.array([0.60, 0.80, 0.96], dtype=float)\n",
    "    df[\"lease_rel_approx\"] = np.interp(df[\"lease_left\"].clip(0, 99), anchors_years, anchors_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vdiRdk8Xm3t"
   },
   "outputs": [],
   "source": [
    "# Loading Aux Data\n",
    "aux_dict = {\n",
    "    \"hdb\": pd.read_csv(hdb_data_path),\n",
    "    \"mrt\": pd.read_csv(mrt_stations_path),\n",
    "    \"pri\": pd.read_csv(pri_schools_path),\n",
    "    \"sec\": pd.read_csv(sec_schools_path),\n",
    "    \"malls\": pd.read_csv(shopping_malls_path),\n",
    "    \"hawkers\": pd.read_csv(gov_hawkers_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEKwfYL9cDED"
   },
   "outputs": [],
   "source": [
    "# HDB Data\n",
    "hdb = aux_dict[\"hdb\"].copy()\n",
    "\n",
    "hdb.columns = hdb.columns.str.lower()\n",
    "\n",
    "for col in [\"town\",\"block\", \"address\", \"subzone\", \"planning_area\", \"region\"]:\n",
    "    if col in hdb.columns: hdb[col] = hdb[col].map(norm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYwGHZOdj8Sa"
   },
   "outputs": [],
   "source": [
    "hdb_pairs = hdb[[\"town\",\"block\"]].drop_duplicates()\n",
    "train_pairs = train_df[[\"town\",\"block\"]]\n",
    "test_pairs  = test_df[[\"town\",\"block\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_8SGzp7lTNf",
    "outputId": "f6d5ce1d-68f4-4a32-fe56-0bc75d803db9"
   },
   "outputs": [],
   "source": [
    "train_cov = train_pairs.merge(hdb_pairs, on=[\"town\",\"block\"], how=\"left\", indicator=True)\n",
    "test_cov = test_pairs.merge(hdb_pairs,  on=[\"town\",\"block\"], how=\"left\", indicator=True)\n",
    "\n",
    "train_match_n = (train_cov[\"_merge\"] == \"both\").sum()\n",
    "test_match_n = (test_cov[\"_merge\"] == \"both\").sum()\n",
    "\n",
    "print(f\"[COVERAGE] Train matches: {train_match_n}/{len(train_cov)} = {train_match_n/len(train_cov)}\")\n",
    "print(f\"[COVERAGE] TEST matches: {test_match_n}/{len(test_cov)} = {test_match_n/len(test_cov)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqjlaDkDmrA9"
   },
   "outputs": [],
   "source": [
    "hdb_unique = hdb.drop_duplicates(subset=[\"town\",\"block\"])\n",
    "\n",
    "cols_to_add = [\"latitude\",\"longitude\",\"max_floor\",\"subzone\",\"planning_area\",\"region\"]\n",
    "cols_to_add = [c for c in cols_to_add if c in hdb_unique.columns]  # guard\n",
    "\n",
    "train_df = train_df.merge(hdb_unique[[\"town\",\"block\", *cols_to_add]], on=[\"town\",\"block\"], how=\"left\")\n",
    "test_df = test_df.merge (hdb_unique[[\"town\",\"block\", *cols_to_add]], on=[\"town\",\"block\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBwthwx0FOXc"
   },
   "outputs": [],
   "source": [
    "# Floor effects (relative position in block)\n",
    "# Higher floors typically command a premium, and “how high” depends on the block’s max floor\n",
    "# Relative floor and an “is high floor” flag. https://ideas.repec.org/a/taf/apeclt/v26y2019i6p436-439.html\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df[\"rel_floor\"] = (df[\"floor_mid\"] / df[\"max_floor\"]).replace([np.inf, -np.inf], np.nan)\n",
    "    df[\"is_high_floor\"] = (df[\"rel_floor\"] >= 0.7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfQTzYH6lfCl"
   },
   "outputs": [],
   "source": [
    "# MRT Data\n",
    "mrt = aux_dict[\"mrt\"].copy()\n",
    "\n",
    "mrt.columns = mrt.columns.str.lower()\n",
    "\n",
    "for col in [\"code\",\"name\", \"status\", \"subzone\", \"planning_area\", \"region\"]:\n",
    "    if col in mrt.columns: mrt[col] = mrt[col].map(norm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GxOWDj-mZYi",
    "outputId": "13a044be-b79d-4588-faaf-83c9f844850c"
   },
   "outputs": [],
   "source": [
    "print(mrt['status'].unique())\n",
    "# Open and Planned MRT proximity features\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "mrt_open = mrt[mrt[\"status\"] == \"OPEN\"].copy()\n",
    "mrt_planned = mrt[mrt[\"status\"] == \"PLANNED\"].copy()\n",
    "\n",
    "R = 6371000.0\n",
    "def to_rad(df): return np.deg2rad(df[[\"latitude\", \"longitude\"]].to_numpy(dtype=float))\n",
    "\n",
    "tree_open = BallTree(to_rad(mrt_open), metric=\"haversine\")\n",
    "tree_planned = BallTree(to_rad(mrt_planned), metric=\"haversine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LHnUHb5p5C_"
   },
   "outputs": [],
   "source": [
    "def add_mrt_features(df, radii_open=(500,1000,2000), radii_plan=(500,1000,2000)):\n",
    "    idx = df[[\"latitude\",\"longitude\"]].dropna().index\n",
    "    X = np.deg2rad(df.loc[idx, [\"latitude\",\"longitude\"]].to_numpy(dtype=float))\n",
    "\n",
    "    # OPEN: nearest + counts\n",
    "    if tree_open is not None and len(idx):\n",
    "        dist, _ = tree_open.query(X, k=1)\n",
    "        df.loc[idx, \"mrt_open_nearest_m\"] = dist[:,0] * R\n",
    "        for r in radii_open:\n",
    "            df.loc[idx, f\"mrt_open_within_{r}m\"] = tree_open.query_radius(X, r/R, count_only=True)\n",
    "    else:\n",
    "        df[\"mrt_open_nearest_m\"] = np.nan\n",
    "        for r in radii_open: df[f\"mrt_open_within_{r}m\"] = 0\n",
    "\n",
    "    # PLANNED: nearest + counts\n",
    "    if tree_planned is not None and len(idx):\n",
    "        dist, _ = tree_planned.query(X, k=1)\n",
    "        df.loc[idx, \"mrt_plan_nearest_m\"] = dist[:,0] * R\n",
    "        for r in radii_plan:\n",
    "            df.loc[idx, f\"mrt_plan_within_{r}m\"] = tree_planned.query_radius(X, r/R, count_only=True)\n",
    "    else:\n",
    "        df[\"mrt_plan_nearest_m\"] = np.nan\n",
    "        for r in radii_plan: df[f\"mrt_plan_within_{r}m\"] = 0\n",
    "\n",
    "    # Comparative signals\n",
    "    df[\"mrt_any_nearest_m\"] = np.nanmin(\n",
    "        np.vstack([df[\"mrt_open_nearest_m\"].to_numpy(dtype=float),\n",
    "                   df[\"mrt_plan_nearest_m\"].to_numpy(dtype=float)]),\n",
    "        axis=0\n",
    "    )\n",
    "    df[\"mrt_plan_closer_than_open\"] = (\n",
    "        (df[\"mrt_plan_nearest_m\"] < df[\"mrt_open_nearest_m\"]).astype(\"Int64\")\n",
    "    )\n",
    "\n",
    "    # fill NA counts for rows without coords\n",
    "    for c in [col for col in df.columns if col.startswith((\"mrt_open_within_\",\"mrt_plan_within_\"))]:\n",
    "        df[c] = df[c].fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_mrt_features(train_df)\n",
    "test_df = add_mrt_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2H50AOHev_g0"
   },
   "outputs": [],
   "source": [
    "# School Data\n",
    "pri = aux_dict[\"pri\"].copy()\n",
    "sec = aux_dict[\"sec\"].copy()\n",
    "\n",
    "pri.columns = pri.columns.str.lower()\n",
    "sec.columns = sec.columns.str.lower()\n",
    "\n",
    "for col in [\"name\",\"street\", \"subzone\", \"planning_area\", \"region\"]:\n",
    "    if col in pri.columns: pri[col] = pri[col].map(norm_key)\n",
    "    if col in sec.columns: sec[col] = sec[col].map(norm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD0h1-fHxXdv"
   },
   "outputs": [],
   "source": [
    "# Distance to school\n",
    "# MOE’s P1 registration use home-school distance categories of <1 km, 1–2 km, >2 km for priority, so those should capture a policy-driven price signal.\n",
    "\n",
    "def add_proximity_to_school(df, tree, prefix, radii=(1000, 2000)):\n",
    "    idx = df[[\"latitude\",\"longitude\"]].dropna().index\n",
    "    X = np.deg2rad(df.loc[idx, [\"latitude\",\"longitude\"]].to_numpy(dtype=float))\n",
    "    # nearest distance (meters)\n",
    "    dist, _ = tree.query(X, k=1)\n",
    "    df.loc[idx, f\"{prefix}_nearest_m\"] = dist[:,0] * R\n",
    "    # counts within radii (meters)\n",
    "    for r in radii:\n",
    "        cnt = tree.query_radius(X, r / R, count_only=True)\n",
    "        df.loc[idx, f\"{prefix}_within_{r}m\"] = cnt\n",
    "    # fill rows without coords\n",
    "    for r in radii:\n",
    "        df[f\"{prefix}_within_{r}m\"] = df.get(f\"{prefix}_within_{r}m\", 0).fillna(0).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wb3B0IxPyYVG"
   },
   "outputs": [],
   "source": [
    "# Primary schools\n",
    "tree_pri = BallTree(to_rad(pri), metric=\"haversine\")\n",
    "train_df = add_proximity_to_school(train_df, tree_pri, \"pri\", radii=(1000, 2000))\n",
    "test_df = add_proximity_to_school(test_df, tree_pri, \"pri\", radii=(1000, 2000))\n",
    "\n",
    "# Secondary schools\n",
    "tree_sec = BallTree(to_rad(sec), metric=\"haversine\")\n",
    "train_df = add_proximity_to_school(train_df, tree_sec, \"sec\", radii=(1000, 2000))\n",
    "test_df = add_proximity_to_school(test_df, tree_sec, \"sec\", radii=(1000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uktos1ts8P0E"
   },
   "outputs": [],
   "source": [
    "mall = aux_dict[\"malls\"].copy()\n",
    "mall.columns = mall.columns.str.lower()\n",
    "for col in [\"name\", \"street\", \"postal_code\", \"latitude\", \"longitude\", \"subzone\", \"planning_area\", \"region\"]:\n",
    "    if col in pri.columns: pri[col] = pri[col].map(norm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb7LkDLKEI55"
   },
   "outputs": [],
   "source": [
    "def add_proximity_to_mall(df, tree, prefix, radii=(1000, 2000)):\n",
    "    idx = df[[\"latitude\",\"longitude\"]].dropna().index\n",
    "    X = np.deg2rad(df.loc[idx, [\"latitude\",\"longitude\"]].to_numpy(dtype=float))\n",
    "    # nearest distance (meters)\n",
    "    dist, _ = tree.query(X, k=1)\n",
    "    df.loc[idx, f\"{prefix}_nearest_m\"] = dist[:,0] * R\n",
    "    # counts within radii (meters)\n",
    "    for r in radii:\n",
    "        cnt = tree.query_radius(X, r / R, count_only=True)\n",
    "        df.loc[idx, f\"{prefix}_within_{r}m\"] = cnt\n",
    "    # fill rows without coords\n",
    "    for r in radii:\n",
    "        df[f\"{prefix}_within_{r}m\"] = df.get(f\"{prefix}_within_{r}m\", 0).fillna(0).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AFBgv9KEI55"
   },
   "outputs": [],
   "source": [
    "tree_mall = BallTree(to_rad(mall), metric=\"haversine\")\n",
    "train_df = add_proximity_to_mall(train_df, tree_mall, \"mall\", radii=(1000, 2000))\n",
    "test_df = add_proximity_to_mall(test_df, tree_mall, \"mall\", radii=(1000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GXKV1k2EI55"
   },
   "outputs": [],
   "source": [
    "hawker = aux_dict[\"hawkers\"].copy()\n",
    "hawker.columns = hawker.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZmYhgt9EI55"
   },
   "outputs": [],
   "source": [
    "def add_proximity_to_hawker(df, tree, prefix, radii=(500, 1000)):\n",
    "    idx = df[[\"latitude\",\"longitude\"]].dropna().index\n",
    "    # nothing to do if no points with coords\n",
    "    if len(idx) == 0:\n",
    "        # ensure columns exist for downstream code\n",
    "        for r in radii:\n",
    "            df[f\"{prefix}_within_{r}m\"] = df.get(f\"{prefix}_within_{r}m\", 0).fillna(0).astype(int)\n",
    "            df[f\"{prefix}_stalls_within_{r}m\"] = df.get(f\"{prefix}_stalls_within_{r}m\", 0).fillna(0).astype(int)\n",
    "        df[f\"{prefix}_nearest_m\"] = np.nan\n",
    "        return df\n",
    "\n",
    "    X = np.deg2rad(df.loc[idx, [\"latitude\",\"longitude\"]].to_numpy(dtype=float))\n",
    "    # nearest distance (meters)\n",
    "    dist, _ = tree.query(X, k=1)\n",
    "    df.loc[idx, f\"{prefix}_nearest_m\"] = dist[:,0] * R\n",
    "\n",
    "    # counts within radii (meters) and sum of stalls within radii\n",
    "    stalls_col = 'number_of_stalls'\n",
    "    for r in radii:\n",
    "        inds = tree.query_radius(X, r / R)\n",
    "        cnt = np.array([len(a) for a in inds])\n",
    "        df.loc[idx, f\"{prefix}_within_{r}m\"] = cnt\n",
    "        stalls_sum = np.array([hawker.iloc[a][stalls_col].sum() if len(a) > 0 else 0 for a in inds])\n",
    "        df.loc[idx, f\"{prefix}_stalls_within_{r}m\"] = stalls_sum\n",
    "\n",
    "    # fill rows without coords and ensure integer types for counts\n",
    "    for r in radii:\n",
    "        df[f\"{prefix}_within_{r}m\"] = df.get(f\"{prefix}_within_{r}m\", 0).fillna(0).astype(int)\n",
    "        df[f\"{prefix}_stalls_within_{r}m\"] = df.get(f\"{prefix}_stalls_within_{r}m\", 0).fillna(0).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwLrJyqYEI55"
   },
   "outputs": [],
   "source": [
    "tree_hawker = BallTree(to_rad(hawker), metric=\"haversine\")\n",
    "train_df = add_proximity_to_hawker(train_df, tree_hawker, \"hawker\", radii=(500, 1000))\n",
    "test_df = add_proximity_to_hawker(test_df, tree_hawker, \"hawker\", radii=(500, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "N7YpuvntEI55",
    "outputId": "8b151b80-56ab-496b-8bba-7287cf232fc4"
   },
   "outputs": [],
   "source": [
    "train_df.groupby(\"hawker_within_500m\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaQsYT5uEI56"
   },
   "source": [
    "### Feature Engineering: Flat Age in Days\n",
    "\n",
    "**Objective:** Create a precise age metric for HDB flats to capture depreciation effects.\n",
    "\n",
    "This feature calculates the exact age of each flat in days from the lease commencement date to the sale date. Age is a crucial factor in property valuation as:\n",
    "- Older flats typically have lower resale values\n",
    "- Age affects remaining lease duration\n",
    "- Newer flats may have modern amenities and better conditions\n",
    "\n",
    "**Formula:** `flat_age_days = sale_date - lease_commence_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "MpyuWkUQEI56",
    "outputId": "e0128162-5217-433d-fc63-8e83083f74b2"
   },
   "outputs": [],
   "source": [
    "def add_flat_age_days(train_df, test_df):\n",
    "\n",
    "    train_df['lease_commence_dt'] = pd.to_datetime(train_df['lease_commence_date'].astype(str) + '-01-01')\n",
    "    test_df['lease_commence_dt'] = pd.to_datetime(test_df['lease_commence_date'].astype(str) + '-01-01')\n",
    "\n",
    "    train_df['flat_age_days'] = (train_df['month'] - train_df['lease_commence_dt']).dt.days\n",
    "    test_df['flat_age_days'] = (test_df['month'] - test_df['lease_commence_dt']).dt.days\n",
    "\n",
    "    train_df = train_df.drop(columns=['lease_commence_dt'])\n",
    "    test_df = test_df.drop(columns=['lease_commence_dt'])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = add_flat_age_days(train_df, test_df)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HteM4ApRGBHP"
   },
   "outputs": [],
   "source": [
    "# One-hot for coarse region, and target/mean encoding for higher-cardinality planning_area/subzone\n",
    "\n",
    "# Region one-hot\n",
    "region_dum = pd.get_dummies(train_df[\"region\"], prefix=\"region\", dummy_na=False)\n",
    "train_df = pd.concat([train_df, region_dum], axis=1)\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df[\"region\"], prefix=\"region\", dummy_na=False)], axis=1)\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)  # align after dummies\n",
    "\n",
    "# KFold mean encoding for planning_area\n",
    "from sklearn.model_selection import KFold\n",
    "def kfold_target_mean_encode(tr, te, col, y=\"resale_price\", n_splits=5, seed=777):\n",
    "    tr = tr.copy(); te = te.copy()\n",
    "    global_mean = tr[y].mean()\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    tr_enc = pd.Series(index=tr.index, dtype=float)\n",
    "    for tr_idx, val_idx in kf.split(tr):\n",
    "        m = tr.iloc[tr_idx].groupby(col)[y].mean()\n",
    "        tr_enc.iloc[val_idx] = tr.iloc[val_idx][col].map(m).fillna(global_mean)\n",
    "    te_enc = te[col].map(tr.groupby(col)[y].mean()).fillna(global_mean)\n",
    "    tr[f\"{col}_te\"] = tr_enc\n",
    "    te[f\"{col}_te\"] = te_enc\n",
    "    return tr, te\n",
    "\n",
    "train_df, test_df = kfold_target_mean_encode(train_df, test_df, \"planning_area\")\n",
    "train_df, test_df = kfold_target_mean_encode(train_df, test_df, \"subzone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M55UsN2KGVHP"
   },
   "outputs": [],
   "source": [
    "# Improving upon flat-type price hierarchy\n",
    "\n",
    "# Hedonic housing interaction\n",
    "for df in (train_df, test_df):\n",
    "    df[\"area_centered\"] = df[\"floor_area_sqm\"] - train_df[\"floor_area_sqm\"].mean()\n",
    "    df[\"exec_x_area\"] = df[\"is_exec\"] * df[\"area_centered\"]\n",
    "    df[\"multigen_x_area\"] = df[\"is_multigen\"] * df[\"area_centered\"]\n",
    "\n",
    "# coarse one-hot for model (limit top N categories to avoid sparsity)\n",
    "top_models = train_df[\"flat_model\"].value_counts().head(8).index\n",
    "for df in (train_df, test_df):\n",
    "    df[\"flat_model_top\"] = df[\"flat_model\"].where(df[\"flat_model\"].isin(top_models), \"OTHER\")\n",
    "model_dum = pd.get_dummies(train_df[\"flat_model_top\"], prefix=\"model\")\n",
    "train_df = pd.concat([train_df, model_dum], axis=1)\n",
    "test_df  = pd.concat([test_df, pd.get_dummies(test_df[\"flat_model_top\"], prefix=\"model\")], axis=1)\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATASET SHAPE AND STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"\\nNumber of features: {train_df.shape[1] - 1}\")  # -1 for target variable\n",
    "print(f\"Number of training samples: {train_df.shape[0]:,}\")\n",
    "print(f\"Number of test samples: {test_df.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*70)\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "missing = train_df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"No missing values in training data!\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TARGET VARIABLE: resale_price\")\n",
    "print(\"=\"*70)\n",
    "print(train_df['resale_price'].describe())\n",
    "print(f\"\\nPrice Range: ${train_df['resale_price'].min():,.0f} - ${train_df['resale_price'].max():,.0f}\")\n",
    "print(f\"Median Price: ${train_df['resale_price'].median():,.0f}\")\n",
    "print(f\"Mean Price: ${train_df['resale_price'].mean():,.0f}\")\n",
    "print(f\"Std Dev: ${train_df['resale_price'].std():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribution of Resale Prices\n",
    "\n",
    "**Analysis:** Understanding the distribution shape helps determine appropriate preprocessing and modeling strategies.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Right-skewed distribution:** Indicates presence of high-value outliers (premium properties)\n",
    "- **Log transformation benefits:** Normalizes the distribution for better model performance\n",
    "- **Central tendency measures:** Mean vs. median comparison reveals skewness impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution of resale prices\n",
    "axes[0].hist(train_df['resale_price'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Resale Prices', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Resale Price ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].axvline(train_df['resale_price'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${train_df[\"resale_price\"].mean():,.0f}')\n",
    "axes[0].axvline(train_df['resale_price'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${train_df[\"resale_price\"].median():,.0f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Log-transformed distribution\n",
    "axes[1].hist(np.log1p(train_df['resale_price']), bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title('Log-Transformed Distribution of Resale Prices', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('log(Resale Price)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Log transformation makes the distribution more Gaussian, which helps with modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation Analysis - Numerical Features\n",
    "\n",
    "**Objective:** Identify the strongest predictors of resale price through correlation analysis.\n",
    "\n",
    "**What to look for:**\n",
    "- **Strong positive correlations (r > 0.5):** Primary price drivers\n",
    "- **Weak correlations (|r| < 0.2):** Features that may have limited predictive power\n",
    "- **Negative correlations:** Features inversely related to price (e.g., distance to amenities)\n",
    "\n",
    "**Note:** Correlation measures linear relationships only. Tree-based models can capture non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key numerical features for correlation analysis\n",
    "key_features = [\n",
    "    'resale_price', 'floor_area_sqm', 'lease_commence_date', \n",
    "    'storey_range', 'sale_year', 'sale_month'\n",
    "]\n",
    "\n",
    "# Get all numerical columns\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = train_df[numerical_cols].corr()['resale_price'].sort_values(ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOP 15 FEATURES MOST CORRELATED WITH RESALE PRICE\")\n",
    "print(\"=\"*70)\n",
    "print(correlations.head(15))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 FEATURES LEAST CORRELATED (MOST NEGATIVE) WITH RESALE PRICE\")\n",
    "print(\"=\"*70)\n",
    "print(correlations.tail(10))\n",
    "\n",
    "# Visualize top correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_corr = correlations.head(20).sort_values()\n",
    "top_corr.plot(kind='barh', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_title('Top 20 Features Correlated with Resale Price', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation Heatmap - Key Features\n",
    "\n",
    "**Purpose:** Visualize relationships between the most important features and identify multicollinearity.\n",
    "\n",
    "**Interpretation Guide:**\n",
    "- **Dark red (close to 1):** Strong positive correlation\n",
    "- **Dark blue (close to -1):** Strong negative correlation\n",
    "- **Light colors (close to 0):** Weak/no correlation\n",
    "- **Off-diagonal patterns:** Feature interactions and multicollinearity\n",
    "\n",
    "**Multicollinearity concerns:**\n",
    "- High correlation between predictors (e.g., floor_area and bedrooms)\n",
    "- May affect linear models but less critical for tree-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select most important features for heatmap\n",
    "important_features = [\n",
    "    'resale_price', 'floor_area_sqm', 'lease_commence_date', \n",
    "    'sale_year', 'flat_type_price_hier', 'bedrooms_est',\n",
    "    'floor_mid', 'lease_left', 'month_index'\n",
    "]\n",
    "\n",
    "# Filter features that exist in the dataframe\n",
    "available_features = [f for f in important_features if f in train_df.columns]\n",
    "\n",
    "if len(available_features) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = train_df[available_features].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    import seaborn as sns\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap of Key Features', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insights from Correlation Analysis:\")\n",
    "    print(\"1. floor_area_sqm likely has strong positive correlation with price\")\n",
    "    print(\"2. lease_left (remaining lease) likely correlates with price\")\n",
    "    print(\"3. flat_type_price_hier captures flat type effect on price\")\n",
    "    print(\"4. Temporal features (sale_year, month_index) may show market trends\")\n",
    "else:\n",
    "    print(\"Not enough features available for correlation heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Categorical Features Analysis\n",
    "\n",
    "**Analysis Focus:** Examine how categorical variables (flat type, location) influence resale prices.\n",
    "\n",
    "**Key Questions:**\n",
    "1. **Flat Type Distribution:** Which flat types are most common in the dataset?\n",
    "2. **Price Hierarchy:** Do larger flat types command premium prices?\n",
    "3. **Location Effects:** How does geographic location (town) impact resale values?\n",
    "4. **Market Concentration:** Are transactions concentrated in specific areas?\n",
    "\n",
    "**Encoding Strategy:**\n",
    "- **Low cardinality (flat_type):** One-hot encoding\n",
    "- **High cardinality (town, planning_area):** Target/mean encoding to prevent feature explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze flat type distribution and average prices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Flat type distribution\n",
    "flat_type_counts = train_df['flat_type'].value_counts()\n",
    "axes[0, 0].barh(flat_type_counts.index, flat_type_counts.values, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Count', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Flat Types', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Average price by flat type\n",
    "avg_price_by_type = train_df.groupby('flat_type')['resale_price'].mean().sort_values()\n",
    "axes[0, 1].barh(avg_price_by_type.index, avg_price_by_type.values, color='coral', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Average Resale Price ($)', fontsize=12)\n",
    "axes[0, 1].set_title('Average Resale Price by Flat Type', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Town distribution (top 15)\n",
    "town_counts = train_df['town'].value_counts().head(15)\n",
    "axes[1, 0].barh(town_counts.index, town_counts.values, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Count', fontsize=12)\n",
    "axes[1, 0].set_title('Top 15 Towns by Number of Transactions', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Average price by town (top 15)\n",
    "avg_price_by_town = train_df.groupby('town')['resale_price'].mean().sort_values(ascending=False).head(15)\n",
    "axes[1, 1].barh(avg_price_by_town.index, avg_price_by_town.values, color='gold', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Average Resale Price ($)', fontsize=12)\n",
    "axes[1, 1].set_title('Top 15 Towns by Average Resale Price', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"1. Most common flat type: {flat_type_counts.index[0]}\")\n",
    "print(f\"2. Highest average price flat type: {avg_price_by_type.index[-1]} (${avg_price_by_type.values[-1]:,.0f})\")\n",
    "print(f\"3. Most transactions in town: {town_counts.index[0]}\")\n",
    "print(f\"4. Highest average price town: {avg_price_by_town.index[0]} (${avg_price_by_town.values[0]:,.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Temporal Trends - Price Evolution Over Time\n",
    "\n",
    "**Objective:** Analyze how HDB resale prices have changed over time and identify market trends.\n",
    "\n",
    "**Analysis Components:**\n",
    "1. **Year-over-year price trends:** Identify appreciation/depreciation patterns\n",
    "2. **Transaction volume trends:** Market activity levels over time\n",
    "3. **Seasonal effects:** Price variations by quarter/month (if any)\n",
    "\n",
    "**Modeling Implications:**\n",
    "- Increasing prices --> Include temporal features (year, month_index)\n",
    "- Market cycles --> Polynomial/interaction terms for sale year\n",
    "- Volume changes --> May indicate market sentiment shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# 1. Average price by year\n",
    "yearly_avg = train_df.groupby('sale_year')['resale_price'].agg(['mean', 'median', 'count'])\n",
    "axes[0].plot(yearly_avg.index, yearly_avg['mean'], marker='o', linewidth=2, label='Mean Price', color='blue')\n",
    "axes[0].plot(yearly_avg.index, yearly_avg['median'], marker='s', linewidth=2, label='Median Price', color='green')\n",
    "axes[0].set_xlabel('Year', fontsize=12)\n",
    "axes[0].set_ylabel('Resale Price ($)', fontsize=12)\n",
    "axes[0].set_title('Average Resale Price Trend Over Years', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Transaction volume by year\n",
    "axes[1].bar(yearly_avg.index, yearly_avg['count'], color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Year', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Transactions', fontsize=12)\n",
    "axes[1].set_title('Transaction Volume Over Years', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Temporal Analysis:\")\n",
    "print(yearly_avg)\n",
    "print(f\"\\nPrice increase from {yearly_avg.index.min()} to {yearly_avg.index.max()}: \"\n",
    "      f\"${yearly_avg['mean'].iloc[-1] - yearly_avg['mean'].iloc[0]:,.0f} \"\n",
    "      f\"({((yearly_avg['mean'].iloc[-1] / yearly_avg['mean'].iloc[0]) - 1) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Key Feature Relationships - Scatter Plots\n",
    "\n",
    "**Purpose:** Visualize bivariate relationships between continuous features and resale price.\n",
    "\n",
    "**Patterns to Identify:**\n",
    "1. **Linear relationships:** Constant rate of change (e.g., floor area vs. price)\n",
    "2. **Non-linear patterns:** Diminishing/accelerating returns\n",
    "3. **Heteroscedasticity:** Variance changes with feature values\n",
    "4. **Outliers and anomalies:** Unusual data points requiring investigation\n",
    "\n",
    "**Model Selection Insights:**\n",
    "- Linear patterns → Linear regression may suffice\n",
    "- Non-linear patterns → Tree-based models or polynomial features needed\n",
    "- Complex interactions → Gradient boosting recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots to visualize relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Sample data for faster plotting\n",
    "sample_df = train_df.sample(n=min(5000, len(train_df)), random_state=777)\n",
    "\n",
    "# 1. Floor area vs Price\n",
    "axes[0, 0].scatter(sample_df['floor_area_sqm'], sample_df['resale_price'], alpha=0.3, s=10)\n",
    "axes[0, 0].set_xlabel('Floor Area (sqm)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Resale Price ($)', fontsize=12)\n",
    "axes[0, 0].set_title('Floor Area vs Resale Price', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Lease commence year vs Price\n",
    "axes[0, 1].scatter(sample_df['lease_commence_date'], sample_df['resale_price'], alpha=0.3, s=10, color='green')\n",
    "axes[0, 1].set_xlabel('Lease Commence Year', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Resale Price ($)', fontsize=12)\n",
    "axes[0, 1].set_title('Lease Commence Year vs Resale Price', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Floor level vs Price (if available)\n",
    "if 'floor_mid' in sample_df.columns:\n",
    "    axes[1, 0].scatter(sample_df['floor_mid'], sample_df['resale_price'], alpha=0.3, s=10, color='orange')\n",
    "    axes[1, 0].set_xlabel('Floor Level (mid)', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Resale Price ($)', fontsize=12)\n",
    "    axes[1, 0].set_title('Floor Level vs Resale Price', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Remaining lease vs Price (if available)\n",
    "if 'lease_left' in sample_df.columns:\n",
    "    axes[1, 1].scatter(sample_df['lease_left'], sample_df['resale_price'], alpha=0.3, s=10, color='red')\n",
    "    axes[1, 1].set_xlabel('Remaining Lease (years)', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Resale Price ($)', fontsize=12)\n",
    "    axes[1, 1].set_title('Remaining Lease vs Resale Price', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Relationships Observed:\")\n",
    "print(\"1. Floor area shows strong positive correlation with price\")\n",
    "print(\"2. Newer flats (higher lease commence year) tend to have higher prices\")\n",
    "print(\"3. Higher floors may command premium prices\")\n",
    "print(\"4. Remaining lease length affects property valuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. EDA Summary and Key Findings for Report\n",
    "\n",
    "**Purpose:** Synthesize all EDA findings into actionable insights for model development.\n",
    "\n",
    "This summary section provides:\n",
    "- **Quantitative findings:** Key statistics and metrics\n",
    "- **Qualitative insights:** Patterns and relationships discovered\n",
    "- **Feature importance:** Which variables matter most\n",
    "- **Preprocessing recommendations:** Data transformations needed\n",
    "- **Model selection rationale:** Why certain algorithms are preferred\n",
    "\n",
    "**Use this summary for:**\n",
    "- Report writing (Results and Discussion sections)\n",
    "- Model selection justification\n",
    "- Feature engineering decisions\n",
    "- Performance expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY - KEY FINDINGS FOR REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n  1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   - Total training samples: {train_df.shape[0]:,}\")\n",
    "print(f\"   - Number of features: {train_df.shape[1] - 1}\")\n",
    "print(f\"   - No missing values in dataset\")\n",
    "print(f\"   - Price range: ${train_df['resale_price'].min():,.0f} - ${train_df['resale_price'].max():,.0f}\")\n",
    "print(f\"   - Median price: ${train_df['resale_price'].median():,.0f}\")\n",
    "\n",
    "print(\"\\n  2. STRONGEST PREDICTORS (Top Correlations):\")\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlations = train_df[numerical_cols].corr()['resale_price'].sort_values(ascending=False)\n",
    "print(f\"   - Floor area (correlation: {correlations.get('floor_area_sqm', 'N/A'):.3f})\")\n",
    "print(f\"   - Flat type hierarchy\")\n",
    "print(f\"   - Remaining lease years\")\n",
    "print(f\"   - Location (town, planning area)\")\n",
    "print(f\"   - Floor level\")\n",
    "\n",
    "print(\"\\n  3. TEMPORAL TRENDS:\")\n",
    "yearly_avg = train_df.groupby('sale_year')['resale_price'].mean()\n",
    "print(f\"   - Prices show {'upward' if yearly_avg.iloc[-1] > yearly_avg.iloc[0] else 'downward'} trend over time\")\n",
    "print(f\"   - Market shows temporal patterns requiring time-based features\")\n",
    "\n",
    "print(\"\\n  4. CATEGORICAL FEATURES INSIGHTS:\")\n",
    "print(f\"   - {len(train_df['town'].unique())} unique towns\")\n",
    "print(f\"   - {len(train_df['flat_type'].unique())} flat types\")\n",
    "print(f\"   - Significant price variation across locations (high cardinality)\")\n",
    "print(f\"   - Flat type strongly influences price\")\n",
    "\n",
    "print(\"\\n 5. FEATURE INTERACTIONS:\")\n",
    "print(\"   - Floor area x Flat type: Larger flats of premium types command higher prices\")\n",
    "print(\"   - Location x Time: Different areas show different appreciation rates\")\n",
    "print(\"   - Lease remaining x Sale year: Depreciation effects on older flats\")\n",
    "\n",
    "print(\"\\n 6. DATA PREPROCESSING REQUIREMENTS:\")\n",
    "print(\"   - Log transformation beneficial for price distribution\")\n",
    "print(\"   - One-hot encoding needed for low-cardinality categorical features\")\n",
    "print(\"   - Target/mean encoding for high-cardinality features (town, planning area)\")\n",
    "print(\"   - Feature engineering: temporal, proximity, and interaction features\")\n",
    "\n",
    "print(\"\\n 7. MODELING IMPLICATIONS:\")\n",
    "print(\"   - Non-linear relationships suggest tree-based models will perform well\")\n",
    "print(\"   - High feature dimensionality after encoding → gradient boosting preferred\")\n",
    "print(\"   - Temporal patterns --> include time-based features\")\n",
    "print(\"   - Location importance --> proximity features to amenities valuable\")\n",
    "\n",
    "print(\"\\n 8. WHY POLYNOMIAL REGRESSION IS SUBOPTIMAL:\")\n",
    "print(\"   - High-dimensional feature space (100+ features after encoding)\")\n",
    "print(\"   - Polynomial expansion creates 1000s of features--> overfitting risk\")\n",
    "print(\"   - Non-linear patterns better captured by tree-based models\")\n",
    "print(\"   - Computational cost: O(n^2) for degree 2, O(n^3) for degree 3\")\n",
    "print(\"   - XGBoost/LightGBM capture interactions automatically without explosion\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION: Gradient Boosting (XGBoost/LightGBM) is optimal for this problem\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7gIQGTJEI56"
   },
   "source": [
    "---\n",
    "## 🤖 Baseline Models - Performance Benchmarking\n",
    "\n",
    "This section implements various regression algorithms to establish performance baselines and compare model effectiveness.\n",
    "\n",
    "**Models Evaluated:**\n",
    "1. Linear Regression (simplest baseline)\n",
    "2. Polynomial Regression (degree 2, with optimization)\n",
    "3. Decision Tree Regression\n",
    "4. Bagging Regressor\n",
    "5. Random Forest Regressor\n",
    "6. **XGBoost (best performer)**\n",
    "\n",
    "**Evaluation Metric:** Root Mean Squared Error (RMSE) on validation set (20% holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8IPJGwbEI56"
   },
   "source": [
    "### Model 1: Linear Regression (Baseline)\n",
    "\n",
    "**Algorithm:** Ordinary Least Squares (OLS) Linear Regression\n",
    "\n",
    "**Characteristics:**\n",
    "- Simplest interpretable model\n",
    "- Assumes linear relationships between features and target\n",
    "- Fast training and prediction\n",
    "- Works well with properly scaled features\n",
    "\n",
    "**Expected Performance:** ~55,000 RMSE (highest error among models)\n",
    "\n",
    "**Limitations:**\n",
    "- Cannot capture non-linear relationships\n",
    "- Sensitive to outliers\n",
    "- Assumes feature independence (multicollinearity issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqrObFdHEI56",
    "outputId": "bcad259b-1a28-4c2e-eb05-19e22c5ed95c"
   },
   "outputs": [],
   "source": [
    "target = 'resale_price'\n",
    "\n",
    "# Define columns that should NOT be used as features\n",
    "# block and street are in lat lon\n",
    "# month and lease_commence_date are in flat_age_days\n",
    "drop_cols = [target, 'month', 'block', 'street', 'lease_commence_date']\n",
    "\n",
    "\n",
    "numerical_features = [\n",
    "    col for col in train_df.select_dtypes(include=np.number).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    col for col in train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())                    # Normalize data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df[target]\n",
    "\n",
    "# Create a validation split (80% train, 20% validation)\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=777\n",
    ")\n",
    "\n",
    "model_pipeline.fit(X_train_split, y_train_split)\n",
    "val_predictions = model_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "validation_mse = mean_squared_error(y_val, val_predictions)\n",
    "validation_rmse = np.sqrt(validation_mse)\n",
    "\n",
    "print(f\"Validation RMSE: {validation_rmse:.4f}\")\n",
    "\n",
    "# retraining on all data\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "# Save the entire pipeline (preprocessor + model) to a file\n",
    "model_filename = 'linear_regression.joblib'\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "# Validation RMSE: 55385.9387\n",
    "# ['linear_regression.joblib']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYBxQPHbEI56"
   },
   "source": [
    "### Model 2: Polynomial Regression (Degree 2) - Optimized\n",
    "\n",
    "**Algorithm:** Ridge Regression with Polynomial Features (degree=2, interaction_only=True)\n",
    "\n",
    "**Optimization Strategies:**\n",
    "1. **Feature Selection:** SelectKBest limits to top 30 numerical features\n",
    "2. **Interaction Terms Only:** Avoids feature explosion (no squared terms)\n",
    "3. **Sparse Matrices:** Memory-efficient representation\n",
    "4. **Ridge Regularization:** Handles multicollinearity better than OLS\n",
    "5. **LSQR Solver:** Compatible with all scipy versions\n",
    "\n",
    "**Why interaction_only=True?**\n",
    "- Full polynomial expansion creates O(n^2) features\n",
    "- With 100+ features, degree 2 creates 5,000+ features -> overfitting\n",
    "- Interaction terms capture relationships without explosion\n",
    "\n",
    "**Trade-off:** Better than linear but slower training. Still inferior to tree-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2Nd1hidEI56",
    "outputId": "9eb8bb80-fd2c-49ee-ee2a-49b31ab1ea4c"
   },
   "outputs": [],
   "source": [
    "# Optimized Polynomial Regression (CPU) - using feature selection and sparse matrices\n",
    "target = 'resale_price'\n",
    "drop_cols = [target, 'month', 'block', 'street', 'lease_commence_date']\n",
    "\n",
    "numerical_features = [\n",
    "    col for col in train_df.select_dtypes(include=np.number).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    col for col in train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "# Strategy 1: Reduce features before polynomial expansion\n",
    "# Select only the most important numerical features to reduce computational burden\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "print(f\"Original numerical features: {len(numerical_features)}\")\n",
    "\n",
    "# Use a subset of features for faster polynomial expansion\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectKBest(f_regression, k=min(30, len(numerical_features))))  # Limit to top 30 features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # Keep sparse\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.3  # Use sparse matrices when beneficial\n",
    ")\n",
    "\n",
    "# Strategy 2: Use interaction_only=True to reduce feature explosion\n",
    "# This creates only interaction terms, not powers (much fewer features)\n",
    "from sklearn.linear_model import Ridge  # Ridge handles multicollinearity better\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),  # Much faster!\n",
    "    ('model', Ridge(alpha=1.0))  # Ridge is more stable than OLS for many features\n",
    "])\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df[target]\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=777\n",
    ")\n",
    "\n",
    "print(\"Training optimized polynomial model (interaction_only=True)...\")\n",
    "print(\"This should be much faster than full polynomial expansion!\")\n",
    "\n",
    "model_pipeline.fit(X_train_split, y_train_split)\n",
    "\n",
    "val_predictions = model_pipeline.predict(X_val)\n",
    "\n",
    "validation_mse = mean_squared_error(y_val, val_predictions)\n",
    "validation_rmse = np.sqrt(validation_mse)\n",
    "\n",
    "print(f\"Validation RMSE: {validation_rmse:.4f}\")\n",
    "\n",
    "# Retrain on full data\n",
    "print(\"Retraining on full dataset...\")\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "model_filename = 'polynomial_regression_optimized.joblib'\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "print(f\"\\nOptimization strategies used:\")\n",
    "print(\"  1. Feature selection: Reduced to top 30 numerical features\")\n",
    "print(\"  2. interaction_only=True: Only interaction terms, no powers\")\n",
    "print(\"  3. Sparse matrices: Memory efficient representation\")\n",
    "print(\"  4. Ridge regression: Better regularization than OLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Boc57RA_EI57"
   },
   "source": [
    "### Model 3: Decision Tree Regression\n",
    "\n",
    "**Algorithm:** Single Decision Tree (CART)\n",
    "\n",
    "**Characteristics:**\n",
    "- Non-linear model that splits feature space recursively\n",
    "- Can capture complex relationships and interactions automatically\n",
    "- No feature scaling required\n",
    "- Prone to overfitting without regularization\n",
    "\n",
    "**Expected Performance:** ~42,000 RMSE\n",
    "\n",
    "**Limitations:**\n",
    "- High variance (unstable predictions)\n",
    "- Overfits easily on training data\n",
    "- Not ensemble method -> lower accuracy than Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sSBYR9mEI57",
    "outputId": "4cf3a0fb-7667-4891-f0b0-3a2e0e12a29d"
   },
   "outputs": [],
   "source": [
    "target = 'resale_price'\n",
    "\n",
    "drop_cols = [target, 'month', 'block', 'street', 'lease_commence_date']\n",
    "\n",
    "numerical_features = [\n",
    "    col for col in train_df.select_dtypes(include=np.number).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    col for col in train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())                    # Normalize data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeRegressor(random_state=777)) # Apply Decision Tree model\n",
    "])\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df[target]\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=777\n",
    ")\n",
    "\n",
    "model_pipeline.fit(X_train_split, y_train_split)\n",
    "\n",
    "val_predictions = model_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "validation_mse = mean_squared_error(y_val, val_predictions)\n",
    "validation_rmse = np.sqrt(validation_mse)\n",
    "\n",
    "print(f\"Validation RMSE: {validation_rmse:.4f}\")\n",
    "\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "model_filename = 'decision_tree.joblib'\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "# Validation RMSE: 42634.5061\n",
    "# ['decision_tree.joblib']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2gtQAcPEI57"
   },
   "source": [
    "### Model 4: Bagging Regressor\n",
    "\n",
    "**Algorithm:** Bootstrap Aggregating (Bagging) with Decision Trees\n",
    "\n",
    "**How it Works:**\n",
    "1. Create multiple bootstrap samples from training data\n",
    "2. Train independent decision trees on each sample\n",
    "3. Average predictions from all trees\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces variance compared to single decision tree\n",
    "- Parallel training of trees (faster than boosting)\n",
    "- Less prone to overfitting than single tree\n",
    "\n",
    "**Expected Performance:** ~31,500 RMSE\n",
    "\n",
    "**Limitation:** Trees trained independently (doesn't learn from previous errors like boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h2gSeaHEI57",
    "outputId": "a1de10fc-2150-4a21-f77f-a650b829a97a"
   },
   "outputs": [],
   "source": [
    "target = 'resale_price'\n",
    "\n",
    "drop_cols = [target, 'month', 'block', 'street', 'lease_commence_date']\n",
    "\n",
    "numerical_features = [\n",
    "    col for col in train_df.select_dtypes(include=np.number).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    col for col in train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())                    # Normalize data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', BaggingRegressor(random_state=777)) # Apply Bagging Regression model\n",
    "])\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df[target]\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=777\n",
    ")\n",
    "\n",
    "model_pipeline.fit(X_train_split, y_train_split)\n",
    "\n",
    "val_predictions = model_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "validation_mse = mean_squared_error(y_val, val_predictions)\n",
    "validation_rmse = np.sqrt(validation_mse)\n",
    "\n",
    "print(f\"Validation RMSE: {validation_rmse:.4f}\")\n",
    "\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "model_filename = 'bagging.joblib'\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "# Validation RMSE: 31510.9685\n",
    "# ['bagging.joblib']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wdpl_hfEI57"
   },
   "source": [
    "### Model 5: Random Forest Regression\n",
    "\n",
    "**Algorithm:** Ensemble of Decision Trees with Random Feature Selection\n",
    "\n",
    "**How it Improves on Bagging:**\n",
    "1. Bootstrap sampling (like bagging)\n",
    "2. **Random feature subset** at each split → decorrelates trees\n",
    "3. Averages predictions from many diverse trees\n",
    "\n",
    "**Advantages:**\n",
    "- Lower variance than bagging\n",
    "- Feature importance scores\n",
    "- Robust to outliers and missing values\n",
    "- Works well out-of-the-box\n",
    "\n",
    "**Expected Performance:** ~29,800 RMSE\n",
    "\n",
    "**Why Still Inferior to XGBoost?**\n",
    "- Random Forest: Trees built independently\n",
    "- XGBoost: Trees built sequentially, learning from errors → better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X00pBbZb0Agh"
   },
   "outputs": [],
   "source": [
    "target = 'resale_price'\n",
    "\n",
    "drop_cols = [target, 'month', 'block', 'street', 'lease_commence_date']\n",
    "\n",
    "numerical_features = [\n",
    "    col for col in train_df.select_dtypes(include=np.number).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    col for col in train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    if col not in drop_cols\n",
    "]\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())                    # Normalize data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=777)) # Apply Random Forest Regression model\n",
    "])\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df[target]\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=777\n",
    ")\n",
    "\n",
    "model_pipeline.fit(X_train_split, y_train_split)\n",
    "\n",
    "val_predictions = model_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "validation_mse = mean_squared_error(y_val, val_predictions)\n",
    "validation_rmse = np.sqrt(validation_mse)\n",
    "\n",
    "print(f\"Validation RMSE: {validation_rmse:.4f}\")\n",
    "\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "model_filename = 'random_forest.joblib'\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "# Validation RMSE: 29868.7537\n",
    "# ['random_forest.joblib']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j54DyTU5J9RJ"
   },
   "source": [
    "### Model 6: XGBoost (Extreme Gradient Boosting) - Best Model \n",
    "\n",
    "**Algorithm:** Gradient Boosting with Advanced Regularization\n",
    "\n",
    "**Why XGBoost is Superior:**\n",
    "1. **Sequential learning:** Each tree corrects errors of previous trees\n",
    "2. **Regularization:** L1 (alpha) and L2 (lambda) prevent overfitting\n",
    "3. **Advanced tree pruning:** Max depth, min child weight, gamma\n",
    "4. **Column/row subsampling:** Reduces overfitting and training time\n",
    "5. **Built-in cross-validation:** Early stopping based on validation performance\n",
    "\n",
    "**Hyperparameter Tuning Strategy:**\n",
    "- **40 random search trials** (good balance of exploration vs. time)\n",
    "- **Log-transformed target:** More Gaussian residuals, better handling of outliers\n",
    "- **Early stopping (200 rounds):** Prevents overtraining\n",
    "- **Validation RMSE:** ~25,400 (best among all models)\n",
    "\n",
    "**Implementation Details:**\n",
    "- GPU acceleration enabled (if available)\n",
    "- Saves best model automatically to `models/xgb_best/`\n",
    "- Returns preprocessor + model for easy deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh9TautOqx1n"
   },
   "outputs": [],
   "source": [
    "def train_xgboost_model(\n",
    "    train_df,\n",
    "    target: str = \"resale_price\",\n",
    "    drop_cols: Optional[List[str]] = None,\n",
    "    numerical_features: Optional[List[str]] = None,\n",
    "    categorical_features: Optional[List[str]] = None,\n",
    "    val_size: float = 0.2,\n",
    "    random_state: int = 777,\n",
    "    use_gpu: bool = True,\n",
    "    n_trials: int = 30,\n",
    "    early_stopping_rounds: int = 200,\n",
    "    max_boost_rounds: int = 5000,\n",
    "    save_dir: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    # Initialize random number generator for hyperparameter sampling\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "\n",
    "    # Column prep\n",
    "    drop_cols = [target, 'month', 'block', 'street', 'lease_commence_date']\n",
    "\n",
    "    X = train_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    y = train_df[target].to_numpy()\n",
    "\n",
    "    if numerical_features is None:\n",
    "        numerical_features = list(X.select_dtypes(include=np.number).columns)\n",
    "    if categorical_features is None:\n",
    "        categorical_features = list(X.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "\n",
    "    if not numerical_features and not categorical_features:\n",
    "        raise ValueError(\"No features found. Check drop_cols and dtypes.\")\n",
    "\n",
    "    # Preprocessor\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), numerical_features),\n",
    "            (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe)]), categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train_raw, y_val_raw = train_test_split(\n",
    "        X, y, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train_t = preprocessor.fit_transform(X_train)\n",
    "    X_val_t = preprocessor.transform(X_val)\n",
    "\n",
    "    # Log-transform target for more Gaussian residuals, this should help with outliers\n",
    "    y_train = np.log1p(y_train_raw)\n",
    "    y_val = np.log1p(y_val_raw)\n",
    "\n",
    "    def make_params(overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"max_depth\": 8,\n",
    "            \"min_child_weight\": 2.0,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"alpha\": 0.0, # L1\n",
    "            \"lambda\": 1.0, # L2\n",
    "            \"gamma\": 0.0,\n",
    "            \"grow_policy\": \"lossguide\",\n",
    "        }\n",
    "        if use_gpu:\n",
    "            params.update({\"device\": \"cuda\", \"tree_method\": \"hist\"})\n",
    "        else:\n",
    "            params.update({\"tree_method\": \"hist\"})\n",
    "        if overrides:\n",
    "            lr = overrides.pop(\"learning_rate\", None)\n",
    "            if lr is not None:\n",
    "                params[\"eta\"] = lr\n",
    "            params.update(overrides)\n",
    "        return params\n",
    "\n",
    "    # Random sampling\n",
    "    def sample_params() -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"learning_rate\": 10 ** rng.uniform(-2.3, -0.7), # ~0.005..0.2\n",
    "            \"max_depth\": int(rng.integers(4, 13)), # 4..12\n",
    "            \"min_child_weight\": float(10 ** rng.uniform(-0.3, 1.0)), # ~0.5..10\n",
    "            \"subsample\": float(rng.uniform(0.6, 1.0)),\n",
    "            \"colsample_bytree\": float(rng.uniform(0.6, 1.0)),\n",
    "            \"alpha\": float(10 ** rng.uniform(-3, 1)), # 0.001..10\n",
    "            \"lambda\": float(10 ** rng.uniform(-2, 1)), # 0.01..10\n",
    "            \"gamma\": float(10 ** rng.uniform(-3, 0)), # 0.001..1\n",
    "            \"max_leaves\": int(rng.integers(16, 512)),\n",
    "        }\n",
    "\n",
    "    # Helpers to use \"best trees\"\n",
    "    def best_ntrees(booster: xgb.Booster) -> int:\n",
    "        if hasattr(booster, \"best_iteration\") and booster.best_iteration is not None:\n",
    "            return int(booster.best_iteration) + 1\n",
    "        if hasattr(booster, \"best_ntree_limit\") and booster.best_ntree_limit:\n",
    "            return int(booster.best_ntree_limit)\n",
    "        return max_boost_rounds\n",
    "\n",
    "    def predict_best(booster: xgb.Booster, dmat: xgb.DMatrix) -> np.ndarray:\n",
    "        nbest = best_ntrees(booster)\n",
    "        try:\n",
    "            return booster.predict(dmat, iteration_range=(0, nbest))\n",
    "        except TypeError:\n",
    "            return booster.predict(dmat, ntree_limit=nbest)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_t, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val_t, label=y_val)\n",
    "\n",
    "    # Hyperparameter search\n",
    "    best: Dict[str, Any] = {\"rmse\": math.inf, \"params\": None, \"best_iteration\": None, \"booster\": None}\n",
    "\n",
    "    pbar = tqdm(range(n_trials), unit=\"trial\", desc=\"XGB hyperparam search\")\n",
    "    for _ in pbar:\n",
    "        trial = sample_params()\n",
    "        params = make_params(trial)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=max_boost_rounds,\n",
    "            evals=[(dval, \"val\")],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        # score on original scale\n",
    "        val_pred = np.expm1(predict_best(booster, dval))\n",
    "        rmse = float(np.sqrt(mean_squared_error(y_val_raw, val_pred)))\n",
    "\n",
    "        if rmse < best[\"rmse\"]:\n",
    "            best.update({\n",
    "                \"rmse\": rmse,\n",
    "                \"params\": params,\n",
    "                \"best_iteration\": best_ntrees(booster) - 1,  # store 0-based\n",
    "                \"booster\": booster,\n",
    "            })\n",
    "        pbar.set_postfix(rmse=f\"{rmse:.2f}\", best=f\"{best['rmse']:.2f}\")\n",
    "\n",
    "    # Refit with best params\n",
    "    X_full_t = preprocessor.transform(X)\n",
    "    y_full = np.log1p(y)\n",
    "    dfull = xgb.DMatrix(X_full_t, label=y_full)\n",
    "\n",
    "    final_params = dict(best[\"params\"])\n",
    "    final_rounds = int(best[\"best_iteration\"]) + 1\n",
    "\n",
    "    bst_final = xgb.train(\n",
    "        params=final_params,\n",
    "        dtrain=dfull,\n",
    "        num_boost_round=final_rounds,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    # Adapter for the prediction\n",
    "    class BoosterAdapter:\n",
    "        def __init__(self, booster: xgb.Booster, ntrees: int):\n",
    "            self.booster = booster\n",
    "            self.best_iteration = ntrees - 1\n",
    "        def predict(self, Xmat) -> np.ndarray:\n",
    "            dmat = xgb.DMatrix(Xmat)\n",
    "            try:\n",
    "                return self.booster.predict(dmat, iteration_range=(0, self.best_iteration + 1))\n",
    "            except TypeError:\n",
    "                return self.booster.predict(dmat, ntree_limit=self.best_iteration + 1)\n",
    "        def __repr__(self) -> str:\n",
    "            return f\"BoosterAdapter(best_iteration={self.best_iteration})\"\n",
    "\n",
    "    model_adapter = BoosterAdapter(bst_final, final_rounds)\n",
    "\n",
    "    if save_dir:\n",
    "        path = Path(save_dir)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(preprocessor, path / \"xgb_model.joblib\")\n",
    "        bst_final.save_model(str(path / \"xgb_model.json\"))\n",
    "        with (path / \"best_summary.json\").open(\"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"val_rmse\": best[\"rmse\"],\n",
    "                    \"best_iteration\": int(best[\"best_iteration\"]),\n",
    "                    \"num_boost_round\": final_rounds,\n",
    "                    \"best_params\": {\n",
    "                        k: final_params[k]\n",
    "                        for k in [\n",
    "                            \"eta\", \"max_depth\", \"min_child_weight\", \"subsample\",\n",
    "                            \"colsample_bytree\", \"alpha\", \"lambda\", \"gamma\",\n",
    "                            \"grow_policy\", *([\"device\"] if \"device\" in final_params else []),\n",
    "                            \"tree_method\",\n",
    "                        ]\n",
    "                        if k in final_params\n",
    "                    },\n",
    "                },\n",
    "                f,\n",
    "                indent=2,\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"preprocessor\": preprocessor,\n",
    "        \"model\": model_adapter,\n",
    "        \"best_params\": final_params,\n",
    "        \"best_iteration\": int(best[\"best_iteration\"]),\n",
    "        \"val_rmse\": float(best[\"rmse\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfnTY5sfruX_"
   },
   "outputs": [],
   "source": [
    "result = train_xgboost_model(\n",
    "    train_df,\n",
    "    target=\"resale_price\",\n",
    "    drop_cols=[\"resale_price\", \"month\", \"block\", \"street\", \"lease_commence_date\"],\n",
    "    save_dir=\"models/xgb_best\",\n",
    "    n_trials=40,\n",
    "    use_gpu=True,\n",
    ")\n",
    "print(f\"Best val RMSE: {result['val_rmse']:.2f}\")\n",
    "\n",
    "# Best val RMSE = 25434.09\n",
    "# ['xgb_model.joblib']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMA9L15xEI57"
   },
   "source": [
    "---\n",
    "## Model Deployment and Predictions\n",
    "\n",
    "**Final Step:** Generate predictions on test set using the best-performing XGBoost model.\n",
    "\n",
    "**Process:**\n",
    "1. Load trained preprocessor and model\n",
    "2. Transform test data using same preprocessing pipeline\n",
    "3. Generate predictions (remember to inverse log-transform)\n",
    "4. Create submission file in required format\n",
    "\n",
    "**Output:** `submission.csv` ready for Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqBn8q1llVEh"
   },
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('C:\\\\Users\\\\GoCh661\\\\Downloads\\\\cs5228\\\\models\\\\xgb_best\\\\xgb_model.joblib') #please change based on your path\n",
    "X_sub = test_df.drop(columns=[\"resale_price\", \"month\", \"block\", \"street\", \"lease_commence_date\"], errors=\"ignore\")\n",
    "X_sub_t = result[\"preprocessor\"].transform(X_sub)\n",
    "final_predictions = np.expm1(result[\"model\"].predict(X_sub_t))\n",
    "results_df = pd.DataFrame({'Id': test_df.index,\n",
    "                           'Predicted': final_predictions})\n",
    "results_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
